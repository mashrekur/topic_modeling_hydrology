{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw corpus dataframe\n",
    "# load cleaned corpus\n",
    "with open('data/data_lemmatized.pkl', 'rb') as f:\n",
    "    data_lemmatized = pkl.load(f)\n",
    "with open('data/cleaned_corpus.pkl', 'rb') as f:\n",
    "    corpus = pkl.load(f)\n",
    "with open(\"data/id2word.pkl\", 'rb') as f:\n",
    "    id2word= pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_topics = 40\n",
    "min_topics = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load perplexity and coherence scores for plotting\n",
    "# with open(\"data/perplexity_recalculated.pkl\", 'rb') as f:\n",
    "#     perplexity = pkl.load(f)\n",
    "# with open(\"data/coherence_recalculated.pkl\", 'rb') as f:\n",
    "#     coherence = pkl.load(f)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Topics = 2: Perplexity = -7.609894813951374, Coherence = 0.36438118953575294\n",
      "Num Topics = 3: Perplexity = -7.546708886077306, Coherence = 0.3819799345514476\n",
      "Num Topics = 4: Perplexity = -7.50943619181426, Coherence = 0.41022892485924534\n",
      "Num Topics = 5: Perplexity = -7.486081360161937, Coherence = 0.4220901894918227\n",
      "Num Topics = 6: Perplexity = -7.476420829759688, Coherence = 0.4410712057346193\n",
      "Num Topics = 7: Perplexity = -7.4749713620413845, Coherence = 0.4837496161680563\n",
      "Num Topics = 8: Perplexity = -7.503085750994842, Coherence = 0.48865928083578536\n",
      "Num Topics = 9: Perplexity = -7.539730749325506, Coherence = 0.47519965489049176\n",
      "Num Topics = 10: Perplexity = -7.5848152260721164, Coherence = 0.47974381799874644\n",
      "Num Topics = 11: Perplexity = -7.623699217763337, Coherence = 0.49272180287774303\n",
      "Num Topics = 12: Perplexity = -7.662205717441724, Coherence = 0.49866675574650166\n"
     ]
    }
   ],
   "source": [
    "# init storage\n",
    "perplexity = {}\n",
    "coherence = {}\n",
    "\n",
    "for topics in range(min_topics, max_topics):\n",
    "    \n",
    "    # load the model\n",
    "    try: \n",
    "        fname = f'trained_models/trained_lda_model_{topics}'\n",
    "        lda_model = LdaModel.load(fname)\n",
    "    except:\n",
    "        print(f'No trained model for {topics} topics')\n",
    "        continue\n",
    "\n",
    "    #Compute Perplexity\n",
    "    perplexity[topics] = lda_model.log_perplexity(corpus) \n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence[topics] = coherence_model_lda.get_coherence()\n",
    "\n",
    "    with open(\"data/perplexity_recalculated.pkl\", 'wb') as f:\n",
    "        pkl.dump(perplexity, f)\n",
    "    with open(\"data/coherence_recalculated.pkl\", 'wb') as f:\n",
    "        pkl.dump(coherence, f)         \n",
    "\n",
    "    # screen report\n",
    "    print(f\"Num Topics = {topics}: Perplexity = {perplexity[topics]}, Coherence = {coherence[topics]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load perplexity and coherence scores for plotting\n",
    "with open(\"data/perplexity_recalculated.pkl\", 'rb') as f:\n",
    "    perplexity = pkl.load(f)\n",
    "with open(\"data/coherence_recalculated.pkl\", 'rb') as f:\n",
    "    coherence = pkl.load(f)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coherence and perplexity scores\n",
    "\n",
    "# grab colors\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "\n",
    "# init figure\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "# plot perplexity\n",
    "lists = sorted(perplexity.items())\n",
    "x, y = zip(*lists) \n",
    "pltp = ax.plot(x, y, label='perplexity', linewidth=5) \n",
    "\n",
    "# plot coherence\n",
    "ax2 = ax.twinx() \n",
    "lists = sorted(coherence.items())\n",
    "x, y = zip(*lists) \n",
    "pltc = ax2.plot(x, y, label='coherence', linewidth=5, color = colors[2])\n",
    "\n",
    "# axis labels\n",
    "ax.set_xlabel('Number of Topics', fontsize=14)\n",
    "ax.set_ylabel('Perplexity', fontsize=14)\n",
    "ax2.set_ylabel('Coherence', fontsize=14)\n",
    "ax.set_title('Finding the Optimal Number of Topics', fontsize=20)\n",
    "\n",
    "# legend\n",
    "ax.legend(pltp+pltc, ['Perplexity', 'Coherence'], fontsize=14, loc='upper left')\n",
    "\n",
    "# aesthetics\n",
    "ax.grid()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('figures/perplexity_coherence_logscale.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
